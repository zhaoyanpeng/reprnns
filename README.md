## Reading List on Theoretical Properties of Neural Sequence Models

On the Computational Power of Transformers and Its Implications in Sequence Modeling, [paper](https://arxiv.org/abs/2006.09286), [code]().

Theoretical Limitations of Self-Attention in Neural Sequence Models, [paper](https://arxiv.org/abs/1906.06755), [code]().

A Formal Hierarchy of RNN Architectures, [paper](https://arxiv.org/abs/2004.08500), [code]().

A single-layer RNN can approximate stacked and bidirectional RNNs, and topologies in between, [paper](https://arxiv.org/abs/1909.00021), [code]().

Rational Recurrences, [paper](https://arxiv.org/abs/1808.09357), [code]().

On the Practical Computational Power of Finite Precision RNNs for Language Recognition, [paper](https://arxiv.org/abs/1805.04908), [code]().

Recurrent Neural Networks as Weighted Language Recognizers, [paper](https://arxiv.org/abs/1711.05408), [code]().

Expressive power of recurrent neural networks, [paper](https://arxiv.org/abs/1711.00811), [code]().
